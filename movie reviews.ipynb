{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MOVIES REVIEWS AND SENTIMENT ANALYSIS DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import urllib\n",
    "import urllib2\n",
    "import nltk\n",
    "import re\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from __future__ import division\n",
    "import io\n",
    "import numpy as np\n",
    "import html2text\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from collections import Counter\n",
    "from operator import itemgetter\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movies reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The movie review are a list of html files saved in a local folder. The first step is to get all the file names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename_list = []\n",
    "#direct = \"file:///C:\\Users\\s161328\\Downloads\\movie\\\\\"\n",
    "direct = \"/Users/Ferran/Downloads/movie/\"\n",
    "for f in listdir(direct):\n",
    "    if f.endswith(\".html\"):\n",
    "        filename_list.append(re.search(r'(.*?)\\.html', f).group(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the title of each movie is extracted. In this database, some titles have a reversed order (e.g. \"Godfather, The\" instead of \"The Godfather\"). In this cases, the title order is exchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "articles_list = [\"The\", \"A\", \"Les\", \"Il\", \"El\", \"Eles\", \"La\", \"Los\", \"Las\", \"De\", \"Das\", \"Den\", \"Die\", \"Det\", \"Der\", \n",
    "                 \"An\", \"Le\", \"L'\", \"Os\", \"Una\", \"O\", \"al-\", \"Lo\"]\n",
    "# Title filtered contains the title for each review, and if necessary, the title is put in the correct order\n",
    "title_filtered = []\n",
    "for f in filename_list:\n",
    "    try:\n",
    "        path = direct + f + \".html\"\n",
    "        \n",
    "        #t = urllib2.urlopen(path).read()\n",
    "        t = codecs.open(path,'r').read()\n",
    "        \n",
    "        soup = BeautifulSoup(t,'html.parser')\n",
    "        # The title of the movies has the format Review for __title__ (__year__)\n",
    "        title = re.search(r'Review for (.+) \\(', soup.title.string).group(1)\n",
    "        try:\n",
    "            # If a title contains a comma, and the right part is contained in articles list, the order is exchanged.\n",
    "            aux = re.search(r'(.+), (.+)', title)\n",
    "            before_comma = aux.group(1)\n",
    "            after_comma = aux.group(2)\n",
    "            if after_coma in articles_list:\n",
    "                title = after_comma + ' ' + before_comma\n",
    "        except:\n",
    "            pass\n",
    "        title_filtered.append(title)\n",
    "    except:\n",
    "        print(\"No title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to extract the review text. In the following part of the code, paragraph with review text are cleaned and joined in a single string. Finally, for the movies that already exist in the database, the list of reviews is added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We start loading the movies dictionary obtained from Wikipedia\n",
    "#with open(\"C:\\Users\\s161328\\Downloads\\movies_dict.txt\") as f:\n",
    "with open(\"movies_dict.txt\") as f:\n",
    "    movies_dict = json.loads(f.read())\n",
    "movies_dict_keys = movies_dict.keys()\n",
    "movies_dict_keys_proc = [re.sub( \":\", \"\", m.lower()) for m in movies_dict_keys]\n",
    "total = 0\n",
    "\n",
    "# We add to each movie, two extra keys, \"Reviews\" and \"Reviews grade\"\n",
    "for movie in movies_dict.keys():\n",
    "    movies_dict[movie][\"Reviews\"] = []\n",
    "    movies_dict[movie][\"Reviews grade\"] = \"\"\n",
    "i = 0\n",
    "\n",
    "for title in title_filtered:\n",
    "    if re.sub(\":\", \"\", title.lower()) in movies_dict_keys_proc:\n",
    "        path = direct + filename_list[i] + \".html\"\n",
    "        #t = urllib2.urlopen(path).read()\n",
    "        t = codecs.open(path,'r').read()\n",
    "        soup = BeautifulSoup(t,'html.parser')\n",
    "        # From each review we extract all paragraph, that contain the movie review\n",
    "        page_paragraphs = [str(x) for x in list(soup.find_all('p'))]\n",
    "        clean_paragraphs = []\n",
    "        # We clean each paragraph, and if it contains the Copyright information, the paragraph is discarded\n",
    "        for paragraph in page_paragraphs:\n",
    "            try:\n",
    "                text = re.search(r'<p>(.+?)</p>', paragraph, re.DOTALL).group(1)\n",
    "                if (re.search(\"Copyright \\d+\",text) == None):\n",
    "                    clean_paragraphs.append(text.strip())\n",
    "            except:\n",
    "                pass \n",
    "        # All paragraphs are joined and cleaned from html characters\n",
    "        review_text = html2text.html2text(\"\\n\".join(clean_paragraphs).decode(\"utf-8\"))\n",
    "        j = movies_dict_keys_proc.index(re.sub(\":\", \"\", title.lower()))\n",
    "        # Reviews texts are added to the dictionary\n",
    "        movies_dict[movies_dict_keys[j]][\"Reviews\"].append(review_text)\n",
    "        i += 1\n",
    "# The resulting dictionary is saved in a txt file\n",
    "json.dump(movies_dict, open(\"movies_dict_with_reviews.txt\",'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, sentiment is calculated based on the reviews added to the dictionary in the previous section. Moreover, instead of using the dataset used in week 7 for calculating sentiment, we will generate the list of most important words for positive and negative reviews (with the associated value) given a list of highly polar reviews (saved in txt files).\n",
    "\n",
    "Some functions have to be defined for calculating sentiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculates the sentiment of a text, given a dictionary that matches words with punctuation\n",
    "def calculate_sentiment(text, sentiment_dict):\n",
    "    token_list = preprocess(text)\n",
    "    sent_keys_lower = [s.lower() for s in sentiment_dict.keys()]\n",
    "    intersect = [i for i in token_list if i in sent_keys_lower] #with repetitons\n",
    "    if (intersect == []): \n",
    "        print \"There are no sentiment words in the token list\"\n",
    "        return 0\n",
    "    return sum([sentiment_dict[i] for i in intersect])/len(token_list)\n",
    "\n",
    "# Processess and tokenizes a text\n",
    "def preprocess(raw):\n",
    "    \n",
    "    #We set the text to lowercase\n",
    "    raw = raw.lower()\n",
    "    \n",
    "    #We keep only the words\n",
    "    tokens = re.findall(r'[a-z]+', raw)\n",
    "    \n",
    "    #We create a exclusion list with the English stopwords\n",
    "    exclusion = stopwords.words('english')\n",
    "    \n",
    "    #We remove the exclusion list from the tokens\n",
    "    filtered_words = [w for w in tokens if not w in exclusion]\n",
    "    \n",
    "    #We remove morphological affixes from words, leaving only the word stem\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    filtered_words_stemmer = [stemmer.stem(t) for t in filtered_words]\n",
    "    \n",
    "    return filtered_words_stemmer\n",
    "\n",
    "#Function that returns a dictionary with the words as a key and their tf-idf value as a value\n",
    "def tfidf(words_counter, num_docs_containing, num_docs):\n",
    "    \n",
    "    #Dictionary with the words as a key and their tf-idf as a value\n",
    "    dict_tfidf = {key: \"\" for key in words_counter.keys()}\n",
    "    \n",
    "    for word in words_counter.keys():\n",
    "    \n",
    "        tf = words_counter[word]\n",
    "        try:\n",
    "            idf = np.log(num_docs/(1 + num_docs_containing[word]))\n",
    "        except KeyError:\n",
    "            idf = np.log(num_docs)\n",
    "        \n",
    "        dict_tfidf[word] = tf * idf\n",
    "        \n",
    "    return dict_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, the text of all reviews is loaded and saved into a list, for both positive and negative reviews. Also, a string with the joined list is saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Negative review is a list containing all negative reviews, and negative_reviews_str joins this list as a string\n",
    "negative_reviews = []\n",
    "#path = \"C:/Users/s161328/Downloads/movies_reviews_classified/neg\"\n",
    "path = \"/Users/Ferran/Downloads/movies_reviews_classified/neg\"\n",
    "for f in listdir(path):\n",
    "    if f.endswith(\".txt\"):\n",
    "        with open(\"%s/%s\" %(path,f)) as neg_review:\n",
    "            # & character caused problems in html2text function (interpreted as EOF). Therefore, it was deleted.\n",
    "            neg_review_clean = re.sub(r\"\\&\",\"\",neg_review.read())\n",
    "            negative_reviews.append(html2text.html2text(neg_review_clean.decode(\"utf-8\")))\n",
    "negative_reviews_str = \" \".join(negative_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Positive review is a list containing all negative reviews, and positive_reviews_str joins this list as a string\n",
    "positive_reviews = []\n",
    "#path = \"C:/Users/s161328/Downloads/movies_reviews_classified/pos\"\n",
    "path = \"/Users/Ferran/Downloads/movies_reviews_classified/pos\"\n",
    "for f in listdir(path):\n",
    "    if f.endswith(\".txt\"):\n",
    "        with open(\"%s/%s\" %(path,f)) as pos_review:\n",
    "            pos_review_clean = re.sub(r\"\\&\",\"\",pos_review.read())\n",
    "            positive_reviews.append(html2text.html2text(pos_review_clean.decode(\"utf-8\")))\n",
    "positive_reviews_str = \" \".join(positive_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use TF-IDF to generate the sentiments words dictionaries, that assigns a value to a word according to the occurrence in positive and negative reviews. For this reason, we need to know, the number of times a word is used in the same-polarity reviews and the number of documents of the opposite polarity that contain this word. With all this information, we build two dictionaries, that give a punctuation to each word as a positive or as a negative review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Processes and tokenizes the string with all the positive reviews\n",
    "positive_words_tokens = preprocess(positive_reviews_str)\n",
    "\n",
    "# Counts the number of docs that contain each word in positive_words_tokens in the list of negative reviews \n",
    "num_docs_containing_pos = Counter()\n",
    "positive_reviews_words_set = set(positive_words_tokens)\n",
    "negative_reviews_set = [set(preprocess(negative_reviews[i])) for i in xrange(len(negative_reviews))]\n",
    "\n",
    "for neg_word_token in negative_reviews_words_set:\n",
    "    for j in xrange(len(positive_reviews)):\n",
    "        if neg_word_token in positive_reviews_set[j]:\n",
    "            num_docs_containing_neg[neg_word_token] += 1\n",
    "json.dump(movies_dict, open(\"num_docs_containing_neg.txt\",'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Processes and tokenizes the string with all the negative reviews\n",
    "negative_words_tokens = preprocess(negative_reviews_str)\n",
    "\n",
    "# Counts the number of docs that contain each word in negative_words_tokens in the list of positive reviews \n",
    "num_docs_containing_neg = Counter()\n",
    "negative_reviews_words_set = set(negative_words_tokens)\n",
    "positive_reviews_set = [set(preprocess(positive_reviews[i])) for i in xrange(len(positive_reviews))]\n",
    "\n",
    "for pos_word_token in positive_reviews_words_set:\n",
    "    for j in xrange(len(negative_reviews)):\n",
    "        if pos_word_token in negative_reviews_set[j]:\n",
    "            num_docs_containing_pos[pos_word_token] += 1\n",
    "json.dump(movies_dict, open(\"num_docs_containing_pos.txt\",'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "positive_words_counter = Counter(positive_words_tokens)\n",
    "json.dump(movies_dict, open(\"positive_words_counter.txt\",'w'))\n",
    "\n",
    "negative_words_counter = Counter(negative_words_tokens)\n",
    "json.dump(movies_dict, open(\"negative_words_counter.txt\",'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Most of the code above takes much time to finish. For this reason we saved resulting dictionaries in txt files.\n",
    "# In this way, we can load the results and continue working another day\n",
    "\n",
    "with open(\"num_docs_containing_neg.txt\") as f:\n",
    "    num_docs_containing_neg = json.loads(f.read())\n",
    "\n",
    "with open(\"num_docs_containing_pos.txt\") as f:\n",
    "    num_docs_containing_pos = json.loads(f.read())\n",
    "    \n",
    "with open(\"positive_words_counter.txt\") as f:\n",
    "    positive_words_counter = json.loads(f.read())\n",
    "\n",
    "with open(\"negative_words_counter.txt\") as f:\n",
    "    negative_words_counter = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculation of both dictionaries, with values for posive and negative words\n",
    "tf_idf_neg = tfidf(negative_words_counter, num_docs_containing_neg, sum(negative_words_counter.values()))\n",
    "tf_idf_pos = tfidf(positive_words_counter, num_docs_containing_pos, sum(positive_words_counter.values()))\n",
    "max_tf_idf_neg = max(tf_idf_neg.values())\n",
    "max_tf_idf_pos = max(tf_idf_pos.values())\n",
    "max_tf_idf = max([max_tf_idf_neg, max_tf_idf_pos])\n",
    "sentiment_dict_neg = {tf_idf:-100*tf_idf_neg[tf_idf]/max_tf_idf for tf_idf in tf_idf_neg.keys()}\n",
    "sentiment_dict_pos = {tf_idf:100*tf_idf_pos[tf_idf]/max_tf_idf for tf_idf in tf_idf_pos.keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the sentiment of each review is calculated as the addition of sentiment in both dictionaries, and the average of sentiment among the reviews in a movie is saved in \"Reviews grade\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"movies_dict_with_reviews.txt\") as f:\n",
    "    movies_dict = json.loads(f.read())\n",
    "# Given both sentiment dictionaries, the sentiment analysis for each movie with reviews is performed and the result is\n",
    "# saved in the \"Reviews grade\" key of the corresponding movie\n",
    "for movie in movies_dict.keys():\n",
    "    num_rev = len(movies_dict[movie][\"Reviews\"])\n",
    "    total_sent = 0\n",
    "    num_empty_rev = 0\n",
    "    if num_rev > 0:\n",
    "        for rev in movies_dict[movie][\"Reviews\"]:\n",
    "            aux = calculate_sentiment(rev, sentiment_dict_pos)\n",
    "            aux2 = calculate_sentiment(rev, sentiment_dict_neg) \n",
    "            # The sentiment of a specific review is the addition of the sentiment of positive and negative words\n",
    "            if(aux + aux2 != 0): total_sent = total_sent + aux + aux2\n",
    "            else: num_empty_rev += 1\n",
    "        movies_dict[movie][\"Reviews grade\"] = total_sent/(num_rev - num_empty_rev)\n",
    "json.dump(movies_dict, open(\"movies_dict_final.txt\",'w'))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
