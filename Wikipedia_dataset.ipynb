{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WIKIPEDIA DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import urllib\n",
    "import urllib2\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in this notebook is used to download movies data from wikipedia and build a dictionary, in order to have a clean dataset.\n",
    "\n",
    "Movies since 1989 are obtained from specific wikipedia pages (that have the structure xxxx_in_film, with xxxx representing the year), since from this year all the pages follow a similar structure. Each of these wikipedia pages contain tables with a list of movies of that year, splitted by the trimester of the release. From these tables, we extract the title and the genre (except for movies from 2004 to 2011 that don't have this info). The other information is obtained from the infobox that each movie has in their wikipedia page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Download a wikipedia page\n",
    "def download_wiki(name):\n",
    "    \n",
    "    # set the parameters (explained in detail here https://www.mediawiki.org/wiki/API:Tutorial)\n",
    "    baseurl = \"https://en.wikipedia.org/w/api.php?\"\n",
    "    action = \"action=query\"\n",
    "    title = \"titles=\"\n",
    "    content = \"prop=revisions&rvprop=content\"\n",
    "    dataformat = \"format=json\"\n",
    "    \n",
    "    # construct the query\n",
    "    query = \"%s%s&%s%s&%s&%s\" % (baseurl,action,title,name.encode('utf-8'),content,dataformat)\n",
    "    print query\n",
    "    wikiresponse = urllib2.urlopen(query)\n",
    "    wikisource = wikiresponse.read()\n",
    "    wikijson = json.loads(wikisource)\n",
    "\n",
    "    try:\n",
    "        key = wikijson[\"query\"][\"pages\"].keys()\n",
    "        text = wikijson[\"query\"][\"pages\"][key[0]][\"revisions\"][0][\"*\"]\n",
    "    except KeyError:\n",
    "        print \"Invalid wikipage name or page does not exist\"\n",
    "        return\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Returns the link part of a wikipedia link\n",
    "def get_movie_link(f):\n",
    "    try:\n",
    "        movie_link = (re.search(r'\\[\\[(.+?)\\]\\]', f).group(1)).replace(\" \", \"_\")\n",
    "        \n",
    "        # If the link contains the character |, we return the right side\n",
    "        if '|' in movie_link:\n",
    "            movie_link = re.search(r'(.+)\\|', movie_link).group(1)\n",
    "        \n",
    "        return movie_link.encode('utf-8')\n",
    "    \n",
    "    # If the provided string does not contain a link, an error message is returned.\n",
    "    except:\n",
    "        error_str = \"Movie without wikipage\"\n",
    "        print error_str\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Returns de label part of a wikipedia link.\n",
    "def get_link_name(f, must_have_link = True):\n",
    "    try:\n",
    "        link_name = re.search(r'\\[\\[(.+?)\\]\\]', f).group(1)\n",
    "        \n",
    "        # If the link contains the character |, we return the right side\n",
    "        if '|' in link_name:\n",
    "            link_name = re.search(r'\\|(.+)', link_name).group(1)\n",
    "        \n",
    "        return link_name.encode('utf-8')\n",
    "    # If the provided string does not contain a link, an error message is returned.\n",
    "    except:\n",
    "        aux = \"\"\n",
    "        if must_have_link == False: aux = f.encode(\"utf-8\")\n",
    "        return aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tries to clean \"string\" with the regular expression \"regex\". If \"regex\" doesn't match the \"string\", the same string is returned\n",
    "def clean_string_regex(string, regex):\n",
    "    try:\n",
    "        return re.search(regex, string, re.DOTALL).group(1)\n",
    "    except:\n",
    "        return string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Returns the image from the infobox of the movie wiki\n",
    "def get_cover_link(link, img_name):\n",
    "    baseurl = \"http://en.wikipedia.org/w/api.php?action=query&titles=\"\n",
    "    # Queries the wikipage for the list of images\n",
    "    generator = \"generator=images&gimlimit=100&prop=imageinfo&iiprop=url|dimensions|mime\"\n",
    "    dataformat = \"format=json\"\n",
    "    query = \"%s%s&%s&%s\" % (baseurl,link.encode('utf-8'),generator,dataformat)\n",
    "    wikijson = json.loads(urllib2.urlopen(query).read())\n",
    "    try:\n",
    "        # Obtains the list of all urls\n",
    "        urls = re.findall(r\"(https?://upload.wikimedia.org/wikipedia/en.+?)\\'\",str(wikijson[\"query\"][\"pages\"]))\n",
    "        # Looks for the images that have the same name as found in the wikipedia infobox\n",
    "        cover_img = list(set([url for url in urls if img_name.lower() in url.lower()]))\n",
    "        if cover_img == []: return \"\"\n",
    "        # If more than one is obtained, the first one is returned\n",
    "        else: cover_img = cover_img[0]\n",
    "            \n",
    "    except KeyError:\n",
    "        print \"Movie without cover image\"\n",
    "        return \"\"\n",
    "    \n",
    "    return cover_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Given a string \"movie_wiki\", extracts the information related to \"atribute\". \"j\" was used for debugging\n",
    "def get_movie_attribute(movie_wiki, attribute, j):\n",
    "    try:\n",
    "        # Extracts the info between the atribute row and the next \"=\"\n",
    "        attr_dirty = re.search(r'\\|[\\s\\t]*%ss?[\\s\\t]*=(.*?)\\n[\\s\\t]*\\|[\\s\\t]*[\\w\\.\\s\\-]+[\\s\\t]*=' % attribute, movie_wiki, re.DOTALL).group(1).strip()\n",
    "    except:\n",
    "        try:\n",
    "            # If the attribute is the last one in the infobox, it looks for the closing curly brackets\n",
    "            attr_dirty = re.search(r'\\|[\\s\\t]*%ss?[\\s\\t]*=(.+?)\\}\\}' % attribute, movie_wiki, re.DOTALL).group(1).strip()          \n",
    "        except:\n",
    "            print str(j+1) + \". Movie does not contain %s\" %attribute\n",
    "            return \"\"\n",
    "   \n",
    "    # Removes all links in [[]] or [[|]] format\n",
    "    try:\n",
    "        attr_clean = re.sub(r'\\[\\[[^\\]]+?\\|([^\\]]+?)\\]\\]', r'\\1', attr_dirty)\n",
    "        attr_clean = re.sub(r'\\[\\[(.*?)\\]\\]', r'\\1', attr_clean, re.DOTALL)\n",
    "    except:\n",
    "        attr_clean = re.sub(r'\\[\\[(.*?)\\]\\]', r'\\1', attr_dirty, re.DOTALL)\n",
    "    \n",
    "    # List of values can be separated by \"*\", \"|\" or <br>\n",
    "    try:\n",
    "        attr_list = re.search(r'^\\{\\{(.+?)\\*(.+)\\}\\}', attr_clean, re.DOTALL).group(2).split(\"*\")\n",
    "    except:\n",
    "        try:\n",
    "            attr_list = re.search(r'^\\{\\{(.+?)\\|(.+)\\}\\}', attr_clean, re.DOTALL).group(2)\n",
    "            attr_list = re.sub(r'\\{\\{(.+?)\\}\\}',\"\", attr_list).split(\"|\")\n",
    "        except:\n",
    "            attr_list = re.split(\"<br.*?>\", attr_clean)\n",
    "\n",
    "    # Only the text on the left side of some special characters (that represent the beginning of a comment, link, citation...)\n",
    "    # is kept.\n",
    "    attr_list = [clean_string_regex(attr, r'(.*?)[<\\(\\{]').strip() for attr in attr_list]\n",
    "    \n",
    "    # Sometimes, in a list, some elements in exclude_list appear and are used to classify elements in a list, but contain no\n",
    "    # relevant information\n",
    "    exclude_list = [\"Animation:\", \"Live-action:\", \"Co-director:\", \"Uncredited:\"]\n",
    "    \n",
    "    # Quotation marks are used to put a text in bold, italics or both. We are not interested in this info, so we delete blocks of\n",
    "    # two ore more consecutive quotation marks (single quotes)\n",
    "    attr_list = [re.sub(r\"\\'\\'+\",\"\",attr).strip() for attr in attr_list]    \n",
    "    attr_list = list(set(attr_list) - set(exclude_list)) \n",
    "    if len(attr_list) == 1: attr_list = attr_list[0]\n",
    "    else:\n",
    "        # We delete the array elements that became empty\n",
    "        attr_list = [attr for attr in attr_list if attr != \"\"]\n",
    "        if len(attr_list) == 1: attr_list = attr_list[0]\n",
    "\n",
    "    return attr_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To flatten the list of movie attributes\n",
    "def flatten_to_strings(listOfLists):\n",
    "    \"\"\"Flatten a list of (lists of (lists of strings)) for any level of nesting\"\"\"\n",
    "    result = []\n",
    "\n",
    "    for i in listOfLists:\n",
    "        # Only append if i is a basestring (superclass of string)\n",
    "        if isinstance(i, basestring):\n",
    "            result.append(i)\n",
    "        # Otherwise call this function recursively\n",
    "        else:\n",
    "            result.extend(flatten_to_strings(i))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extracts the gross of a movie. It can't be done with the function get_movie_attribute as it requires special processing\n",
    "def get_gross(wikitext,j):\n",
    "    total_gross = 0\n",
    "    # All the characters after \"gross\" are obtained\n",
    "    aux = re.search(r\"\\|[\\s\\t]*gross.*\",wikitext, re.DOTALL).group()\n",
    "    # Then, we look for currency symbols (dollar, euro, pound and yen)\n",
    "    movie_gross = [x.decode(\"utf-8\") for x in re.findall(ur\"[\\$\\€\\£\\¥].*\",aux)]\n",
    "    \n",
    "    for elem in movie_gross:\n",
    "        million = False\n",
    "        billion = False\n",
    "        # We delete all the parts of the string in between of \"{}\" or \"<>\"\n",
    "        elem = re.sub(r\"\\{\\{.*?\\}\\}\", r\"\", elem)\n",
    "        elem = re.sub(r\"\\<.*?\\>\", r\"\", elem)\n",
    "        try:\n",
    "            # Currency and value are extracted\n",
    "            aux = re.search(ur\"([\\$\\€\\£\\¥])(.*)\",elem)\n",
    "            currency = aux.group(1)\n",
    "            value = aux.group(2).replace(\",\",\"\").encode(\"utf-8\")\n",
    "            # If the text million or billion apperar, value is multiplied by the corresponding factor\n",
    "            try: \n",
    "                value = re.search(r\"(.*?) ?million\",value).group(1)\n",
    "                million = True\n",
    "            except: pass\n",
    "            try: \n",
    "                value = re.search(r\"(.*?) ?billion\",value).group(1)\n",
    "                billion = True\n",
    "            except: pass\n",
    "            value = float(re.search(r'[\\d\\.]+', value).group())\n",
    "            if million: value *= 1e6\n",
    "            elif billion: value *= 1e9   \n",
    "            # According to the currency, the value is converted to dollars using an approximated exchange rate\n",
    "            if currency in currencies:\n",
    "                if currency == u'\\u20AC': # Euros\n",
    "                    value = value*1.06\n",
    "                elif currency == u'\\u00A3': # Pounds\n",
    "                    value = value*1.24\n",
    "                elif currency == u'\\u00A5': # Yens\n",
    "                    value = value*0.00888\n",
    "            total_gross += value\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "    if total_gross == 0: total_gross = \"\"\n",
    "    return total_gross"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, we obtain the list of movies and their genre from the xxxx_in_film pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extracts table information from wikipages year_in_film. In each of the pages, the table contains for each movie: title, studio,\n",
    "# cast and crew, genre and medium. Only the title and the genre is extracted from each film, as the other parameters will be\n",
    "# obtained from each movie wikipedia page.\n",
    "\n",
    "l = 0\n",
    "year_range = range(1989,2017)\n",
    "movies_list= [\"\"]*len(year_range)\n",
    "movies_names = [\"\"]*len(year_range)\n",
    "genres_list = []\n",
    "\n",
    "path = \"/Users/Ferran/Downloads/year_in_film_final/\"\n",
    "path2 = \"/Users/Ferran/Downloads/year_in_film_title_final/\"\n",
    "for k in year_range:\n",
    "    name = str(k) + \"_in_film\"\n",
    "    text = download_wiki(name)\n",
    "    # The table classifies movies according to the month they were released. Therefore, to obtain it, we look for 3 \"=\"\n",
    "    # (beginning of a subsection) and January. We keep all the characters until we find 2 \"=\" (end of section)\n",
    "    dirty_table = re.search(r'=== ?January(.+?)[\\}\\]]\\n\\n== ?\\w', text, re.DOTALL).group(1)\n",
    "    \n",
    "    # Between 2004 and 2011, there is no \"Genre\" cell\n",
    "    if k in range(2004,2011):\n",
    "        # Each of the cells of the table is obtained\n",
    "        clean_table = re.findall(r\"\\| \\'\\'(.+?)\\|\\|(.*?)\\|\\|(.*?)\\|\", dirty_table, re.DOTALL)\n",
    "        for i in xrange(len(clean_table)): \n",
    "            genres_list.append(\"\")\n",
    "        \n",
    "    else:\n",
    "        clean_table = re.findall(r\"\\| \\'\\'(.+?)\\|\\|(.*?)\\|\\|(.*?)\\|\\|(.*?)\\|\\|(.*?)\\|\", dirty_table, re.DOTALL)\n",
    "        for i in xrange(len(clean_table)):\n",
    "            genres_clean = [get_link_name(genre, must_have_link=False).strip() for genre in clean_table[i][3].split(\",\")]\n",
    "            genres_list.append(genres_clean)\n",
    "\n",
    "    #List with all the movies link part in a year\n",
    "    movies_list[l] = [get_movie_link(clean_table[i][0]) for i in xrange(len(clean_table))]\n",
    "    \n",
    "    #List with all the movies name part in a year\n",
    "    movies_names[l] = [get_link_name(clean_table[i][0]) for i in xrange(len(clean_table))]    \n",
    "        \n",
    "    #Movies names and links saved in files\n",
    "    with open(\"%s%s_in_film.txt\" %(path, k) , \"w\") as text_file:\n",
    "        for movie in movies_list[l]:\n",
    "            print>>text_file, movie.replace(\"/\",\".\")\n",
    "    \n",
    "    with open(\"%s%s_in_film_title.txt\" %(path2, k) , \"w\") as text_file:\n",
    "        for title in movies_names[l]:\n",
    "            print>>text_file, title\n",
    "\n",
    "    l += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we download all movie wikis and save them to text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "j = 0\n",
    "i = 0\n",
    "path = \"/Users/Ferran/Downloads/movie_wikipages/\"\n",
    "for movies_in_year in movies_list:\n",
    "    for movie in movies_list[j]:\n",
    "        try:\n",
    "            # Downloads movie wikis. urllib.quote() is used to convert special characters to % syntax.\n",
    "            movie_wiki = download_wiki(urllib.quote(movie)).encode(\"utf-8\") \n",
    "            if movie_wiki == None: continue\n",
    "            # Each Wikipedia page is saved in a file.\n",
    "            with open(\"%s%s.txt\" % (path,movie.replace(\"/\",\".\")), \"w\") as text_file:\n",
    "                text_file.write(movie_wiki)\n",
    "                \n",
    "        except AttributeError:\n",
    "            print \"Error: \" + str(i+1) + \". \" + movie\n",
    "   \n",
    "        i+=1\n",
    "    j+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we get the movies attributes lists using the functions defined above and the result is saved to a clean dictionary that only contains relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path_year = \"C:\\Users\\s161328\\Downloads\\year_in_film_final\\\\\"\n",
    "#path_year = \"/Users/Ferran/Downloads/year_in_film_final/\"\n",
    "\n",
    "#path_title = \"/Users/Ferran/Downloads/year_in_film_title_final/\"\n",
    "path_title = \"C:\\Users\\s161328\\Downloads\\year_in_film_title_final\\\\\"\n",
    "\n",
    "#path_wikis = \"/Users/Ferran/Downloads/movie_wikipages/\"\n",
    "path_wikis = \"C:\\Users\\s161328\\Downloads\\movie_wikipages\\\\\"\n",
    "\n",
    "year_range = range(1989,2017)\n",
    "movies_dict = dict()\n",
    "\n",
    "i = 0\n",
    "for year in year_range: \n",
    "    f = open('%s%s_in_film.txt' %(path_year, year))\n",
    "    movies_list = f.read().split(\"\\n\")[0:-1]\n",
    "    \n",
    "    g = open('%s%s_in_film_title.txt' %(path_title, year))\n",
    "    movies_names = g.read().split(\"\\n\")[0:-1]\n",
    "    j = 0\n",
    "    currencies = [u'\\u00A3', u'\\u0024', u'\\u00A5', u'\\u20AC']\n",
    "\n",
    "    for movie in movies_list: \n",
    "        try:\n",
    "            wikitext = open('%s%s.txt' % (path_wikis, movie)).read() \n",
    "            total_gross = 0;\n",
    "            # We first extract the infobox area\n",
    "            infobox = re.search(r'[iI]nfobox [fF]ilm(.+?)\\'\\'\\'\\'\\'', wikitext, re.DOTALL).group(1)\n",
    "            # Then we look for each attribute and write it in a dictionary.\n",
    "            if movies_names[j] not in movies_dict.keys():\n",
    "                movies_dict[movies_names[j]] = {\"Director\": get_movie_attribute(infobox, \"director\",j),\n",
    "                                               \"Starring\": get_movie_attribute(infobox, \"starring\",j),\n",
    "                                               \"Language\": get_movie_attribute(infobox, \"language\",j),\n",
    "                                               \"Country\": get_movie_attribute(infobox, \"country\",j),\n",
    "                                               \"Image\": get_cover_link(urllib2.quote(movies_list[j]), get_movie_attribute(infobox, \"image\",j).replace(\" \", \"_\")),\n",
    "                                               \"Year\": year,\n",
    "                                               \"Gross\": get_gross(infobox, j),\n",
    "                                               \"Genre\": genres_list[i]}\n",
    "        except Exception as e:\n",
    "            print e\n",
    "            print str(j+1) + \". Movie without infobox or wikipage does not exist\"\n",
    "        \n",
    "        i += 1\n",
    "        j += 1\n",
    "# The obtained dictionary is save in a txt file\n",
    "json.dump(movies_dict, open(\"movies_dict.txt\", 'w'))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
